{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0441958-5602-4cac-970d-3a057682bca4",
   "metadata": {},
   "source": [
    "# WayFair Analytics Project Part 2 : Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3476fd9e-9b5a-422f-a1f5-44aca7f2506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import shap \n",
    "from sklearn.feature_selection import f_classif\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import ConvergenceWarning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "79a3244a-5066-4418-9501-6550b0acba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset dimensions: 123542 rows and 40 columns\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_Session_Start_Date</th>\n",
       "      <th>Order_ID</th>\n",
       "      <th>Order_Product_ID</th>\n",
       "      <th>Product_ID</th>\n",
       "      <th>Purchased_Qty</th>\n",
       "      <th>Returned_Qty</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Guarantee_Shown</th>\n",
       "      <th>Product_Category</th>\n",
       "      <th>Total_Order_Value</th>\n",
       "      <th>...</th>\n",
       "      <th>Days_Until_Christmas</th>\n",
       "      <th>Is_Pre_Christmas_Week</th>\n",
       "      <th>Est_Delivery_Before_Christmas</th>\n",
       "      <th>Est_Days_From_Christmas</th>\n",
       "      <th>Est_Delivery_Christmas_Period</th>\n",
       "      <th>Customer_Order_Count</th>\n",
       "      <th>Customer_Return_Rate</th>\n",
       "      <th>Customer_Avg_Order_Value</th>\n",
       "      <th>Customer_Avg_Est_Delivery_Days</th>\n",
       "      <th>Customer_Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>56795_22504</td>\n",
       "      <td>2406126045</td>\n",
       "      <td>1743317681120120064</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Heating &amp; Grills</td>\n",
       "      <td>150 - 200</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "      <td>Well After</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112.5</td>\n",
       "      <td>21.5</td>\n",
       "      <td>Value Shoppers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>05922_23349</td>\n",
       "      <td>2268892442</td>\n",
       "      <td>7819236675163079680</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lighting</td>\n",
       "      <td>250 - 300</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-2</td>\n",
       "      <td>Just Before</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>362.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Premium Buyers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-17</td>\n",
       "      <td>12952_23336</td>\n",
       "      <td>2266865412</td>\n",
       "      <td>5928089172568630272</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>80 - 100</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-5</td>\n",
       "      <td>Just Before</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Value Shoppers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-12</td>\n",
       "      <td>82302_23303</td>\n",
       "      <td>2260935702</td>\n",
       "      <td>4719064645483639808</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lighting</td>\n",
       "      <td>40 - 60</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-10</td>\n",
       "      <td>Well Before</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Value Shoppers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-17</td>\n",
       "      <td>70253_23719</td>\n",
       "      <td>1854293603</td>\n",
       "      <td>6657947175955829760</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Seasonal Decor</td>\n",
       "      <td>0 - 20</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Just After</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>117.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Moderate Shoppers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Customer_Session_Start_Date     Order_ID  Order_Product_ID  \\\n",
       "0                  2016-12-18  56795_22504        2406126045   \n",
       "1                  2016-12-19  05922_23349        2268892442   \n",
       "2                  2016-12-17  12952_23336        2266865412   \n",
       "3                  2016-12-12  82302_23303        2260935702   \n",
       "4                  2016-12-17  70253_23719        1854293603   \n",
       "\n",
       "            Product_ID  Purchased_Qty  Returned_Qty  Cancelled  \\\n",
       "0  1743317681120120064              1             0          0   \n",
       "1  7819236675163079680              1             0          0   \n",
       "2  5928089172568630272              1             0          0   \n",
       "3  4719064645483639808              1             0          0   \n",
       "4  6657947175955829760              1             1          0   \n",
       "\n",
       "   Guarantee_Shown  Product_Category Total_Order_Value  ...  \\\n",
       "0                0  Heating & Grills         150 - 200  ...   \n",
       "1                0          Lighting         250 - 300  ...   \n",
       "2                1           Kitchen          80 - 100  ...   \n",
       "3                0          Lighting           40 - 60  ...   \n",
       "4                0    Seasonal Decor            0 - 20  ...   \n",
       "\n",
       "  Days_Until_Christmas Is_Pre_Christmas_Week  Est_Delivery_Before_Christmas  \\\n",
       "0                    7                  True                          False   \n",
       "1                    6                  True                           True   \n",
       "2                    8                 False                           True   \n",
       "3                   13                 False                           True   \n",
       "4                    8                 False                          False   \n",
       "\n",
       "  Est_Days_From_Christmas  Est_Delivery_Christmas_Period Customer_Order_Count  \\\n",
       "0                      26                     Well After                    2   \n",
       "1                      -2                    Just Before                    2   \n",
       "2                      -5                    Just Before                    4   \n",
       "3                     -10                    Well Before                    1   \n",
       "4                       2                     Just After                    2   \n",
       "\n",
       "   Customer_Return_Rate Customer_Avg_Order_Value  \\\n",
       "0                   0.0                    112.5   \n",
       "1                   0.0                    362.5   \n",
       "2                   0.0                     80.0   \n",
       "3                   0.0                     50.0   \n",
       "4                   0.5                    117.5   \n",
       "\n",
       "   Customer_Avg_Est_Delivery_Days   Customer_Segment  \n",
       "0                            21.5     Value Shoppers  \n",
       "1                             4.0     Premium Buyers  \n",
       "2                             7.0     Value Shoppers  \n",
       "3                             3.0     Value Shoppers  \n",
       "4                            10.0  Moderate Shoppers  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in each column:\n",
      "Customer_Actual_Delivery_Date    3194\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-processed dataset\n",
    "df = pd.read_csv('wayfair_part2.csv')\n",
    "\n",
    "# Check dataset dimensions and view sample\n",
    "print(f\"Dataset dimensions: {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Identify missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values_df = missing_values[missing_values > 0]\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81908533-d9f6-42c6-9d33-9d756f50eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns to datetime format\n",
    "date_columns = ['Customer_Session_Start_Date', 'Customer_Estimated_Delivery_Date', 'Customer_Actual_Delivery_Date']\n",
    "for col in date_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c9d27c30-91db-4056-a269-420e64a860a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing Product_Category values\n",
    "if 'Product_Category' in df.columns and df['Product_Category'].isnull().sum() > 0:\n",
    "    df['Product_Category'].fillna('Unknown', inplace=True)\n",
    "    print(\"\\nFilled missing Product_Category values with 'Unknown'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57124cbe-3145-49e5-8f35-aca39fce028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing Order_Value_Numeric values\n",
    "if 'Order_Value_Numeric' in df.columns and df['Order_Value_Numeric'].isnull().sum() > 0:\n",
    "    median_order_value = df['Order_Value_Numeric'].median()\n",
    "    df['Order_Value_Numeric'].fillna(median_order_value, inplace=True)\n",
    "    \n",
    "    # Reconstruct Price_Range from numeric values\n",
    "    if 'Price_Range' in df.columns and df['Price_Range'].isnull().sum() > 0:\n",
    "        def create_price_range(value):\n",
    "            if pd.isna(value):\n",
    "                return None\n",
    "            lower_bound = int(value / 50) * 50\n",
    "            upper_bound = lower_bound + 50\n",
    "            return f\"{lower_bound} - {upper_bound}\"\n",
    "        \n",
    "        mask = df['Price_Range'].isnull()\n",
    "        df.loc[mask, 'Price_Range'] = df.loc[mask, 'Order_Value_Numeric'].apply(create_price_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "82ff62f8-c7ab-4130-ac55-3019b3ef5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing Customer_Estimated_Delivery_Date\n",
    "if 'Customer_Estimated_Delivery_Date' in df.columns and df['Customer_Estimated_Delivery_Date'].isnull().sum() > 0:\n",
    "    # Calculate median Est_Delivery_Days\n",
    "    median_est_days = df['Est_Delivery_Days'].median()\n",
    "    \n",
    "    # Fill missing Est_Delivery_Days\n",
    "    df['Est_Delivery_Days'].fillna(median_est_days, inplace=True)\n",
    "    \n",
    "    # Reconstruct missing dates\n",
    "    mask = df['Customer_Estimated_Delivery_Date'].isnull()\n",
    "    df.loc[mask, 'Customer_Estimated_Delivery_Date'] = df.loc[mask, 'Customer_Session_Start_Date'] + pd.to_timedelta(median_est_days, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4de511a-c740-4676-a30f-48acdb655f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cancelled orders with missing delivery dates: 3194\n"
     ]
    }
   ],
   "source": [
    "# Check for cancelled orders with missing delivery dates\n",
    "if 'Is_Cancelled' in df.columns:\n",
    "    cancelled_with_missing = ((df['Is_Cancelled'] == 1) & \n",
    "                             df['Customer_Actual_Delivery_Date'].isnull()).sum()\n",
    "    print(f\"\\nCancelled orders with missing delivery dates: {cancelled_with_missing}\")\n",
    "    \n",
    "    # Create indicator for non-cancelled missing delivery data\n",
    "    df['Missing_Delivery_Data'] = ((df['Customer_Actual_Delivery_Date'].isnull()) & \n",
    "                                  (df['Is_Cancelled'] != 1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2e07c13-6bad-4b90-b410-1d21f1060d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing Actual_Delivery_Days for non-cancelled orders\n",
    "if 'Actual_Delivery_Days' in df.columns:\n",
    "    # Create mask for non-cancelled orders with missing delivery data\n",
    "    non_cancelled_mask = df['Is_Cancelled'] != 1\n",
    "    missing_delivery_mask = df['Customer_Actual_Delivery_Date'].isnull() & non_cancelled_mask\n",
    "    \n",
    "    # Calculate shipping class-specific medians from valid data\n",
    "    valid_deliveries = df[df['Actual_Delivery_Days'].notnull()]\n",
    "    median_days_by_ship_class = valid_deliveries.groupby('ShipClassName')['Actual_Delivery_Days'].median().to_dict()\n",
    "    \n",
    "    # Apply ship class-specific medians\n",
    "    for ship_class, median_days in median_days_by_ship_class.items():\n",
    "        mask = missing_delivery_mask & (df['ShipClassName'] == ship_class)\n",
    "        df.loc[mask, 'Actual_Delivery_Days'] = median_days\n",
    "    \n",
    "    # Fill any remaining missing values\n",
    "    remaining_missing = df['Actual_Delivery_Days'].isnull() & non_cancelled_mask\n",
    "    if remaining_missing.sum() > 0:\n",
    "        overall_median = valid_deliveries['Actual_Delivery_Days'].median()\n",
    "        df.loc[remaining_missing, 'Actual_Delivery_Days'] = overall_median\n",
    "    \n",
    "    # Reconstruct missing dates for non-cancelled orders\n",
    "    df.loc[missing_delivery_mask, 'Customer_Actual_Delivery_Date'] = (\n",
    "        df.loc[missing_delivery_mask, 'Customer_Session_Start_Date'] +\n",
    "        pd.to_timedelta(df.loc[missing_delivery_mask, 'Actual_Delivery_Days'], unit='D')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0783255-2a30-4a19-a3fb-d0d25ec8d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate Delivery_Delay_Days\n",
    "if 'Delivery_Delay_Days' in df.columns and df['Delivery_Delay_Days'].isnull().sum() > 0:\n",
    "    # For non-cancelled orders with complete dates\n",
    "    non_cancelled_mask = df['Is_Cancelled'] != 1\n",
    "    complete_dates_mask = df['Customer_Actual_Delivery_Date'].notnull() & df['Customer_Estimated_Delivery_Date'].notnull()\n",
    "    \n",
    "    # Recalculate delays\n",
    "    recalc_mask = non_cancelled_mask & complete_dates_mask\n",
    "    df.loc[recalc_mask, 'Delivery_Delay_Days'] = (\n",
    "        df.loc[recalc_mask, 'Customer_Actual_Delivery_Date'] - \n",
    "        df.loc[recalc_mask, 'Customer_Estimated_Delivery_Date']\n",
    "    ).dt.days\n",
    "    \n",
    "    # Fill remaining missing values for non-cancelled orders\n",
    "    still_missing_mask = df['Delivery_Delay_Days'].isnull() & non_cancelled_mask\n",
    "    df.loc[still_missing_mask, 'Delivery_Delay_Days'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f71526de-d527-4bfb-966f-66eed46ab89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Remaining missing values after imputation:\n",
      "Customer_Actual_Delivery_Date    3194\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check remaining missing values after imputation\n",
    "remaining_missing = df.isnull().sum()\n",
    "print(\"\\nRemaining missing values after imputation:\")\n",
    "print(remaining_missing[remaining_missing > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4d197a3b-feb8-49a5-8e0f-6e88469e84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for ML that won't have any missing values\n",
    "df_ml = df.copy()\n",
    "\n",
    "# Create delivery status including cancelled orders\n",
    "df_ml['Delivery_Status'] = np.where(\n",
    "    df_ml['Is_Cancelled'] == 1, 'Cancelled',\n",
    "    np.where(\n",
    "        df_ml['Customer_Actual_Delivery_Date'].isnull(), 'Unknown',\n",
    "        np.where(df_ml['Delivery_Delay_Days'] < 0, 'Early',\n",
    "        np.where(df_ml['Delivery_Delay_Days'] == 0, 'On Time', 'Late'))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set symbolic delivery dates for cancelled orders\n",
    "cancelled_mask = df_ml['Is_Cancelled'] == 1\n",
    "missing_delivery_cancelled = df_ml['Customer_Actual_Delivery_Date'].isnull() & cancelled_mask\n",
    "if missing_delivery_cancelled.sum() > 0:\n",
    "    # Use order date as symbolic delivery date for cancelled orders\n",
    "    df_ml.loc[missing_delivery_cancelled, 'Customer_Actual_Delivery_Date'] = df_ml.loc[missing_delivery_cancelled, 'Customer_Session_Start_Date']\n",
    "    df_ml.loc[missing_delivery_cancelled, 'Actual_Delivery_Days'] = 0\n",
    "    df_ml.loc[missing_delivery_cancelled, 'Delivery_Delay_Days'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a73fe7e-0378-4c82-bf59-9c3a492cfb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all missing values are now handled\n",
    "final_missing = df_ml.isnull().sum()\n",
    "if final_missing.sum() > 0:\n",
    "    print(\"\\nRemaining missing values in ML-ready dataframe:\")\n",
    "    print(final_missing[final_missing > 0])\n",
    "    \n",
    "    # Fill any remaining missing numeric values\n",
    "    numeric_cols = df_ml.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_ml[col].isnull().sum() > 0:\n",
    "            df_ml[col] = df_ml[col].fillna(df_ml[col].median())\n",
    "    \n",
    "    # Fill any remaining missing categorical values\n",
    "    cat_cols = df_ml.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        if df_ml[col].isnull().sum() > 0:\n",
    "            df_ml[col] = df_ml[col].fillna(df_ml[col].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "97058064-9971-4e37-9a05-f51d1e6c1c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add day of month feature\n",
    "if 'Session_Day' not in df_ml.columns:\n",
    "    df_ml['Session_Day'] = df_ml['Customer_Session_Start_Date'].dt.day\n",
    "\n",
    "# Add period of month feature\n",
    "if 'Session_Day_Part' not in df_ml.columns:\n",
    "    df_ml['Session_Day_Part'] = pd.cut(\n",
    "        df_ml['Session_Day'], \n",
    "        bins=[0, 10, 20, 31], \n",
    "        labels=['Early Dec', 'Mid Dec', 'Late Dec']\n",
    "    )\n",
    "\n",
    "# Add Christmas-related features\n",
    "christmas = pd.Timestamp('2016-12-25')\n",
    "if 'Days_Until_Christmas' not in df_ml.columns:\n",
    "    df_ml['Days_Until_Christmas'] = (christmas - df_ml['Customer_Session_Start_Date']).dt.days\n",
    "    df_ml['Is_Pre_Christmas_Week'] = (df_ml['Days_Until_Christmas'] <= 7) & (df_ml['Days_Until_Christmas'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0df5d575-036f-42de-a9c4-2038e6ebe683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer aggregations if not already present\n",
    "if 'Customer_Order_Count' not in df_ml.columns:\n",
    "    customer_features = df_ml.groupby('Customer_ID').agg({\n",
    "        'Order_ID': 'count',\n",
    "        'Has_Return': 'mean',\n",
    "        'Order_Value_Numeric': 'mean',\n",
    "        'Est_Delivery_Days': 'mean',\n",
    "    }).rename(columns={\n",
    "        'Order_ID': 'Customer_Order_Count',\n",
    "        'Has_Return': 'Customer_Return_Rate',\n",
    "        'Order_Value_Numeric': 'Customer_Avg_Order_Value',\n",
    "        'Est_Delivery_Days': 'Customer_Avg_Est_Delivery_Days'\n",
    "    })\n",
    "    \n",
    "    # Merge customer features back to the main dataframe\n",
    "    df_ml = df_ml.merge(customer_features, on='Customer_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ff8e782-1bd6-43d6-8ec3-4b8b43ee742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer segments if not already present\n",
    "if 'Customer_Segment' not in df_ml.columns:\n",
    "    def assign_customer_segment(row):\n",
    "        if row['Customer_Avg_Order_Value'] > 300 and row['Customer_Return_Rate'] < 0.01:\n",
    "            return 'Premium Buyers'\n",
    "        elif row['Customer_Return_Rate'] < 0.01:\n",
    "            return 'Value Shoppers'\n",
    "        elif row['Customer_Return_Rate'] > 0.5:\n",
    "            return 'High-Return Customers'\n",
    "        else:\n",
    "            return 'Moderate Shoppers'\n",
    "    \n",
    "    df_ml['Customer_Segment'] = df_ml.apply(assign_customer_segment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bdee39c4-fd84-4bcf-85cf-a3801c49d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy for encoding\n",
    "df_encoded = df_ml.copy()\n",
    "\n",
    "# Identify categorical columns to encode\n",
    "cat_columns = ['Product_Category', 'Vistor_Type_Name', 'Platform_Name', 'ShipClassName', \n",
    "              'Delivery_Status', 'Customer_Segment', 'Session_Day_Part']\n",
    "cat_columns = [col for col in cat_columns if col in df_ml.columns]\n",
    "\n",
    "# Identify which columns need encoding\n",
    "cat_columns_to_encode = []\n",
    "for col in cat_columns:\n",
    "    if col in df_ml.columns and df_ml[col].dtype == 'object':\n",
    "        if not any(df_ml.columns.str.startswith(f\"{col}_\")):\n",
    "            cat_columns_to_encode.append(col)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "if cat_columns_to_encode:\n",
    "    df_encoded = pd.get_dummies(df_encoded, columns=cat_columns_to_encode, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "244ab409-4bef-4b54-9f31-6be03b1ee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "# Value x Guarantee interaction\n",
    "if 'Value_Category_Interaction' not in df_encoded.columns and 'Has_Guarantee' in df_encoded.columns:\n",
    "    df_encoded['Value_Category_Interaction'] = df_encoded['Order_Value_Numeric'] * df_encoded['Has_Guarantee']\n",
    "\n",
    "# Desktop x Value interaction\n",
    "if 'Desktop_Value_Interaction' not in df_encoded.columns:\n",
    "    if 'Platform_Name_Desktop' in df_encoded.columns:\n",
    "        df_encoded['Desktop_Value_Interaction'] = df_encoded['Order_Value_Numeric'] * df_encoded['Platform_Name_Desktop']\n",
    "    elif 'Platform_ID' in df_encoded.columns:\n",
    "        df_encoded['Desktop_Value_Interaction'] = df_encoded['Order_Value_Numeric'] * (df_encoded['Platform_ID'] == 1).astype(int)\n",
    "\n",
    "# Christmas delivery interaction\n",
    "if 'Christmas_Delivery_Interaction' not in df_encoded.columns and 'Est_Delivery_Before_Christmas' in df_encoded.columns:\n",
    "    df_encoded['Christmas_Delivery_Interaction'] = df_encoded['Est_Delivery_Days'] * df_encoded['Est_Delivery_Before_Christmas'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fae39bc1-af8e-4bdf-b979-de60f7c3ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all available features\n",
    "available_base_features = [\n",
    "    'Purchased_Qty', 'Guarantee_Shown', 'Order_Value_Numeric',\n",
    "    'Est_Delivery_Days', 'Actual_Delivery_Days', 'Delivery_Delay_Days',\n",
    "    'Has_Guarantee', 'Is_Late'\n",
    "]\n",
    "\n",
    "additional_features = [\n",
    "    'Session_Day', 'Days_Until_Christmas', 'Is_Pre_Christmas_Week',\n",
    "    'Est_Delivery_Before_Christmas', 'Est_Days_From_Christmas',\n",
    "    'Customer_Order_Count', 'Customer_Return_Rate', 'Customer_Avg_Order_Value',\n",
    "    'Value_Category_Interaction', 'Desktop_Value_Interaction', 'Christmas_Delivery_Interaction',\n",
    "    'Missing_Delivery_Data'  # Indicator for non-cancelled missing delivery data\n",
    "]\n",
    "\n",
    "# Select features that exist in the dataset\n",
    "base_features = [f for f in available_base_features if f in df_encoded.columns]\n",
    "base_features.extend([f for f in additional_features if f in df_encoded.columns])\n",
    "\n",
    "# Add one-hot encoded features\n",
    "categorical_features = [col for col in df_encoded.columns \n",
    "                       if any(col.startswith(prefix + \"_\") for prefix in cat_columns)]\n",
    "\n",
    "all_features = base_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21b0a289-a1aa-4621-91ca-ab641dab7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target variables from feature list\n",
    "target_vars = ['Has_Return', 'Is_Cancelled', 'Is_Late']\n",
    "features = [f for f in all_features if f not in target_vars]\n",
    "\n",
    "# Define X and y for different prediction tasks\n",
    "X = df_encoded[features]\n",
    "y_returns = df_encoded['Has_Return'] if 'Has_Return' in df_encoded.columns else None\n",
    "y_cancelled = df_encoded['Is_Cancelled'] if 'Is_Cancelled' in df_encoded.columns else None\n",
    "y_late = df_encoded['Is_Late'] if 'Is_Late' in df_encoded.columns else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "87c367f9-3ad8-42d6-b129-bf7e87f1e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up class balancing for imbalanced classification\n",
    "\n",
    "# For binary classification tasks with class imbalance\n",
    "if y_returns is not None:\n",
    "    pos_count = y_returns.sum()\n",
    "    sampling_strategy = {\n",
    "        0: pos_count * 2,  # Keep majority class at 2:1 ratio\n",
    "        1: pos_count       # Keep all minority class\n",
    "    }\n",
    "\n",
    "    # Combined over and under sampling for better balance\n",
    "    resample_pipeline = Pipeline([\n",
    "        ('over', SMOTE(sampling_strategy=0.1, random_state=42)),\n",
    "        ('under', RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=42))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb768c2f-9ab9-439d-b7ae-2b6f4288bced",
   "metadata": {},
   "source": [
    "Modeling\n",
    "\n",
    "Model Selection Strategy : Has Return( 1 = Return, 0 = No return) prediction model \n",
    "\n",
    "We'll implement multiple models for each prediction task and compare their performance. For each task, we'll evaluate:\n",
    "\n",
    "1. Baseline models (LogisticRegression, RandomForest)\n",
    "2. Gradient Boosting models (XGBoost, LightGBM)\n",
    "\n",
    "then choosing the best performance model based on Accuracy prediction rate and AUC rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fde238ec-8cdc-4b31-bc88-f240dfa2e291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression - Accuracy: 0.9741, AUC: 0.9972\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     23257\n",
      "           1       0.69      1.00      0.82      1452\n",
      "\n",
      "    accuracy                           0.97     24709\n",
      "   macro avg       0.85      0.99      0.90     24709\n",
      "weighted avg       0.98      0.97      0.98     24709\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest - Accuracy: 0.9765, AUC: 0.9962\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     23257\n",
      "           1       0.72      0.98      0.83      1452\n",
      "\n",
      "    accuracy                           0.98     24709\n",
      "   macro avg       0.86      0.98      0.91     24709\n",
      "weighted avg       0.98      0.98      0.98     24709\n",
      "\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Gradient Boosting - Accuracy: 0.9762, AUC: 0.9970\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     23257\n",
      "           1       0.71      0.99      0.83      1452\n",
      "\n",
      "    accuracy                           0.98     24709\n",
      "   macro avg       0.86      0.98      0.91     24709\n",
      "weighted avg       0.98      0.98      0.98     24709\n",
      "\n",
      "\n",
      "Training XGBoost...\n",
      "XGBoost - Accuracy: 0.9756, AUC: 0.9969\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     23257\n",
      "           1       0.71      0.99      0.83      1452\n",
      "\n",
      "    accuracy                           0.98     24709\n",
      "   macro avg       0.85      0.98      0.91     24709\n",
      "weighted avg       0.98      0.98      0.98     24709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_returns, test_size=0.2, random_state=42, stratify=y_returns\n",
    ")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply resampling to handle class imbalance (using scaled data)\n",
    "X_train_resampled, y_train_resampled = resample_pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define models to try\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        class_weight='balanced', \n",
    "        max_iter=5000,      # Increased iterations\n",
    "        solver='liblinear', # Different solver\n",
    "        C=0.1,              # Stronger regularization\n",
    "        random_state=42\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        scale_pos_weight=len(y_train_resampled[y_train_resampled==0])/len(y_train_resampled[y_train_resampled==1]),\n",
    "        learning_rate=0.1, \n",
    "        n_estimators=100, \n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    if name == 'Logistic Regression':\n",
    "        # Use scaled data but not resampled data\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "    else:\n",
    "        # Use resampled data for tree-based models\n",
    "        model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions (use scaled test data for all models)\n",
    "    if name == 'Logistic Regression':\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        y_pred = model.predict(X_test_scaled)  # Using scaled test data for all models\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'model': model\n",
    "    }\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9153e-fa54-48af-9947-37f72587269d",
   "metadata": {},
   "source": [
    "Logistic is the best performing model. Now, we can move to intepretation the model output to identify each feature contributing power on the rerturn decision. We will look at p values less than 0.05 and importance or absolute coefficients stats greater than 0.5 that influence on predicting whether an item will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6a7ecacb-15a7-464e-9d80-3a1ce0468492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>AbsCoefficient</th>\n",
       "      <th>Importance</th>\n",
       "      <th>p_value</th>\n",
       "      <th>F_statistic</th>\n",
       "      <th>is_significant</th>\n",
       "      <th>significance</th>\n",
       "      <th>combined_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Customer_Segment_Value Shoppers</td>\n",
       "      <td>-2.637605</td>\n",
       "      <td>2.637605</td>\n",
       "      <td>2.637605</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>59641.082192</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>26376.046741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Customer_Segment_Premium Buyers</td>\n",
       "      <td>-1.760828</td>\n",
       "      <td>1.760828</td>\n",
       "      <td>1.760828</td>\n",
       "      <td>3.644114e-136</td>\n",
       "      <td>618.382516</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>17608.276259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Customer_Return_Rate</td>\n",
       "      <td>1.315377</td>\n",
       "      <td>1.315377</td>\n",
       "      <td>1.315377</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>561264.972288</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>13153.774436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Delivery_Status_On Time</td>\n",
       "      <td>1.225294</td>\n",
       "      <td>1.225294</td>\n",
       "      <td>1.225294</td>\n",
       "      <td>1.381878e-30</td>\n",
       "      <td>132.229031</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>12252.942616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Delivery_Status_Early</td>\n",
       "      <td>0.996367</td>\n",
       "      <td>0.996367</td>\n",
       "      <td>0.996367</td>\n",
       "      <td>1.976860e-15</td>\n",
       "      <td>63.104254</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>9963.670580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Actual_Delivery_Days</td>\n",
       "      <td>0.379566</td>\n",
       "      <td>0.379566</td>\n",
       "      <td>0.379566</td>\n",
       "      <td>1.555761e-59</td>\n",
       "      <td>265.068165</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>3795.657244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Order_Value_Numeric</td>\n",
       "      <td>0.269624</td>\n",
       "      <td>0.269624</td>\n",
       "      <td>0.269624</td>\n",
       "      <td>8.852959e-14</td>\n",
       "      <td>55.619260</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>2696.236911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Customer_Avg_Order_Value</td>\n",
       "      <td>-0.153293</td>\n",
       "      <td>0.153293</td>\n",
       "      <td>0.153293</td>\n",
       "      <td>2.235101e-12</td>\n",
       "      <td>49.275961</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>1532.928181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Customer_Segment_Moderate Shoppers</td>\n",
       "      <td>-0.136679</td>\n",
       "      <td>0.136679</td>\n",
       "      <td>0.136679</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>10910.315486</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>1366.789615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Christmas_Delivery_Interaction</td>\n",
       "      <td>-0.113411</td>\n",
       "      <td>0.113411</td>\n",
       "      <td>0.113411</td>\n",
       "      <td>1.197310e-15</td>\n",
       "      <td>64.092550</td>\n",
       "      <td>True</td>\n",
       "      <td>***</td>\n",
       "      <td>1134.110590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Feature  Coefficient  AbsCoefficient  \\\n",
       "64     Customer_Segment_Value Shoppers    -2.637605        2.637605   \n",
       "63     Customer_Segment_Premium Buyers    -1.760828        1.760828   \n",
       "13                Customer_Return_Rate     1.315377        1.315377   \n",
       "61             Delivery_Status_On Time     1.225294        1.225294   \n",
       "59               Delivery_Status_Early     0.996367        0.996367   \n",
       "4                 Actual_Delivery_Days     0.379566        0.379566   \n",
       "2                  Order_Value_Numeric     0.269624        0.269624   \n",
       "14            Customer_Avg_Order_Value    -0.153293        0.153293   \n",
       "62  Customer_Segment_Moderate Shoppers    -0.136679        0.136679   \n",
       "17      Christmas_Delivery_Interaction    -0.113411        0.113411   \n",
       "\n",
       "    Importance        p_value    F_statistic  is_significant significance  \\\n",
       "64    2.637605   0.000000e+00   59641.082192            True          ***   \n",
       "63    1.760828  3.644114e-136     618.382516            True          ***   \n",
       "13    1.315377   0.000000e+00  561264.972288            True          ***   \n",
       "61    1.225294   1.381878e-30     132.229031            True          ***   \n",
       "59    0.996367   1.976860e-15      63.104254            True          ***   \n",
       "4     0.379566   1.555761e-59     265.068165            True          ***   \n",
       "2     0.269624   8.852959e-14      55.619260            True          ***   \n",
       "14    0.153293   2.235101e-12      49.275961            True          ***   \n",
       "62    0.136679   0.000000e+00   10910.315486            True          ***   \n",
       "17    0.113411   1.197310e-15      64.092550            True          ***   \n",
       "\n",
       "    combined_score  \n",
       "64    26376.046741  \n",
       "63    17608.276259  \n",
       "13    13153.774436  \n",
       "61    12252.942616  \n",
       "59     9963.670580  \n",
       "4      3795.657244  \n",
       "2      2696.236911  \n",
       "14     1532.928181  \n",
       "62     1366.789615  \n",
       "17     1134.110590  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# Function for robust feature importance analysis\n",
    "def analyze_robust_feature_importance(model, X, y, feature_names, model_name):\n",
    "    \"\"\"\n",
    "    Analyze feature importance with robust error handling and statistical testing\n",
    "    \"\"\"\n",
    "    # Initialize figure with adequate size\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Extract model-based importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # For tree-based models\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Create dataframe\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': [feature_names[i] for i in indices],\n",
    "            'Importance': importances[indices]\n",
    "        })\n",
    "        \n",
    "    elif hasattr(model, 'coef_'):\n",
    "        # For linear models like Logistic Regression\n",
    "        coefficients = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Coefficient': model.coef_[0]\n",
    "        })\n",
    "        coefficients['AbsCoefficient'] = np.abs(coefficients['Coefficient'])\n",
    "        importance_df = coefficients.sort_values('AbsCoefficient', ascending=False)\n",
    "        importance_df['Importance'] = importance_df['AbsCoefficient']  # For consistency\n",
    "    else:\n",
    "        print(\"Model does not provide feature importance\")\n",
    "        return None\n",
    "    \n",
    "    # Statistical testing for all features\n",
    "    significant_features = []\n",
    "    p_values = []\n",
    "    f_stats = []\n",
    "    \n",
    "    for i, feature in enumerate(feature_names):\n",
    "        try:\n",
    "            # Skip constant features\n",
    "            if X.iloc[:, i].nunique() <= 1:\n",
    "                p_values.append(1.0)  # Not significant\n",
    "                f_stats.append(0.0)\n",
    "                continue\n",
    "                \n",
    "            # Extract feature values\n",
    "            X_col = X.iloc[:, i].values.reshape(-1, 1)\n",
    "            \n",
    "            # Perform F-test for classification\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                f_stat, p_value = f_classif(X_col, y)\n",
    "                \n",
    "            # Store results\n",
    "            p_values.append(p_value[0])\n",
    "            f_stats.append(f_stat[0])\n",
    "            \n",
    "            # Track significant features\n",
    "            if p_value[0] < 0.05:\n",
    "                significant_features.append(feature)\n",
    "        except Exception as e:\n",
    "            print(f\"Error testing feature {feature}: {e}\")\n",
    "            p_values.append(np.nan)\n",
    "            f_stats.append(np.nan)\n",
    "    \n",
    "    # Add statistical metrics to importance dataframe\n",
    "    importance_df['p_value'] = [p_values[feature_names.index(feature)] if feature in feature_names else np.nan \n",
    "                              for feature in importance_df['Feature']]\n",
    "    \n",
    "    importance_df['F_statistic'] = [f_stats[feature_names.index(feature)] if feature in feature_names else np.nan \n",
    "                                  for feature in importance_df['Feature']]\n",
    "    \n",
    "    # Add significance indicators\n",
    "    importance_df['is_significant'] = importance_df['p_value'] < 0.05\n",
    "    \n",
    "    def significance_stars(p):\n",
    "        if pd.isna(p):\n",
    "            return \"\"\n",
    "        elif p < 0.001:\n",
    "            return \"***\"\n",
    "        elif p < 0.01:\n",
    "            return \"**\"\n",
    "        elif p < 0.05:\n",
    "            return \"*\"\n",
    "        else:\n",
    "            return \"\"\n",
    "    \n",
    "    importance_df['significance'] = importance_df['p_value'].apply(significance_stars)\n",
    "    \n",
    "    # Create a combined importance metric that considers both model importance and statistical significance\n",
    "    importance_df['combined_score'] = importance_df['Importance'] * (1 / (importance_df['p_value'] + 0.0001))\n",
    "    importance_df = importance_df.sort_values('combined_score', ascending=False)\n",
    "    \n",
    "    # Create a version of the dataframe that prioritizes statistically significant features\n",
    "    significant_df = importance_df[importance_df['is_significant']].copy()\n",
    "    if len(significant_df) < 5:  # Ensure we have enough features to analyze\n",
    "        significant_df = importance_df.head(10)\n",
    "    return significant_df\n",
    "# Top 10 features based on p values and importance score (Abscoefficient)\n",
    "significant_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f212b3c-c8ff-4c9c-94fb-76b93f5e6d70",
   "metadata": {},
   "source": [
    "From the output, we can identify that: Value Shoppers and Premium Shoopers are 2 k mean clustering (Part 1) segments influcens on the return probability. Along with the return rata and the delivery status of On Time and Early."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
